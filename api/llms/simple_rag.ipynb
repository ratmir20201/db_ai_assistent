{
 "cells": [
  {
   "cell_type": "code",
   "id": "d0daa44c-229e-42a8-8cba-b0ae4306f70c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T10:48:43.520424Z",
     "start_time": "2025-05-29T10:48:43.514972Z"
    }
   },
   "source": [
    "import ollama\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_ollama import OllamaEmbeddings"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "53b84878-6d0c-4acb-9ffa-7fd981fa7d46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T10:48:49.704589Z",
     "start_time": "2025-05-29T10:48:46.990458Z"
    }
   },
   "source": [
    "# Load the document\n",
    "loader = CSVLoader(file_path=r'C:\\Users\\test\\Desktop\\DM_METADATA.csv', encoding='utf-8')\n",
    "documents = loader.load()\n",
    "\n",
    "# Split the document into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=30, separator=\"\\n\")\n",
    "docs = text_splitter.split_documents(documents=documents)"
   ],
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error loading C:\\Users\\test\\Desktop\\DM_METADATA.csv",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\db_ai_assistent\\.venv\\Lib\\site-packages\\langchain_community\\document_loaders\\csv_loader.py:134\u001B[39m, in \u001B[36mCSVLoader.lazy_load\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    133\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m134\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfile_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnewline\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m csvfile:\n\u001B[32m    135\u001B[39m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m.__read_file(csvfile)\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\test\\\\Desktop\\\\DM_METADATA.csv'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Load the document\u001B[39;00m\n\u001B[32m      2\u001B[39m loader = CSVLoader(file_path=\u001B[33mr\u001B[39m\u001B[33m'\u001B[39m\u001B[33mC:\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mUsers\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mtest\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mDesktop\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mDM_METADATA.csv\u001B[39m\u001B[33m'\u001B[39m, encoding=\u001B[33m'\u001B[39m\u001B[33mutf-8\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m documents = \u001B[43mloader\u001B[49m\u001B[43m.\u001B[49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[38;5;66;03m# Split the document into chunks\u001B[39;00m\n\u001B[32m      6\u001B[39m text_splitter = CharacterTextSplitter(chunk_size=\u001B[32m1000\u001B[39m, chunk_overlap=\u001B[32m30\u001B[39m, separator=\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\db_ai_assistent\\.venv\\Lib\\site-packages\\langchain_core\\document_loaders\\base.py:32\u001B[39m, in \u001B[36mBaseLoader.load\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m     30\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mload\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> \u001B[38;5;28mlist\u001B[39m[Document]:\n\u001B[32m     31\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Load data into Document objects.\"\"\"\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m32\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mlazy_load\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\db_ai_assistent\\.venv\\Lib\\site-packages\\langchain_community\\document_loaders\\csv_loader.py:151\u001B[39m, in \u001B[36mCSVLoader.lazy_load\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    149\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mError loading \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.file_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01me\u001B[39;00m\n\u001B[32m    150\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m--> \u001B[39m\u001B[32m151\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mError loading \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.file_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01me\u001B[39;00m\n",
      "\u001B[31mRuntimeError\u001B[39m: Error loading C:\\Users\\test\\Desktop\\DM_METADATA.csv"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f54f7583-54ce-4440-be48-6458bbb0ad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_embeddings = OllamaEmbeddings(model='nomic-embed-text')\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=docs, embedding=local_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b96b303-4dee-4e7a-8ca2-ea2b63ba1064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# context = \"\"\"Ты — экспертный помощник по Аналитическому центру.\n",
    "# Твоя задача — помогать пользователю работать с базой данных: объяснять таблицы, генерировать SQL-запросы, оптимизировать их, находить ошибки и предлагать улучшения.\n",
    "# Отвечай четко, коротко и на русском языке. Если генерируешь SQL-запрос — пиши его в блоке кода. Если непонятно, что хочет пользователь — задай уточняющий вопрос.\n",
    "\n",
    "# Описание Аналитического центра\n",
    "# База данных: Vertica 24.1.0\n",
    "# Схемы:\n",
    "#   STAGE_DO - Временное или первичное хранилище «сырых» данных, загруженных из источников.\n",
    "#   DWH - Хранилище данных\n",
    "#   DM - Витрины данных\n",
    "#   SANDBOX - Песочница: изолированная среда, где аналитики, дата-сайентисты и разработчики могут экспериментировать с данными, не нарушая основную архитектуру хранилища.\n",
    "\n",
    "# Модель данных DWH - Data Vault 2. Нейминг таблиц в DWH: Начинается на h - Hub, начинается на l - link, начинается на s - satellite.\"\"\"\n",
    "# question = \"\"\"Where is the info about Contracts?\"\"\"\n",
    "# docs = vectorstore.similarity_search(question, 50)\n",
    "# len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ebc732-bcd1-40b0-bebd-bc62a79edf31",
   "metadata": {},
   "source": [
    "DEFINE PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cd43a0e-1ab1-4422-bc95-7ba98a074947",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['llama3.1:latest',\n",
    "          'deepseek-r1:latest',\n",
    "          'mistral:latest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "935de1cc-351a-42ca-a60c-a04ccda1bc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = ['What is the purpose of the table DWH.S_LEGAL_ENTITY_GENERAL?',\n",
    "             'How can I calculate the number of records in the table DWH.S_LEGAL_ENTITY_GENERAL?',\n",
    "             'How can I retrieve the latest LOAD_DATE in the table DWH.S_LEGAL_ENTITY_GENERAL starting from the date 2024-12-31?',\n",
    "             'Give the name of the client with the largest total contract amount starting from the date 2024-12-31.',\n",
    "             'Where are the clients of Baiterek stored?',\n",
    "             'How can I output the number of contracts broken down by DO?',\n",
    "             'Which link is responsible for connecting contracts and clients?',\n",
    "             'Where is the information about contracts stored?',\n",
    "             'Where is the information about clients stored?',\n",
    "             'Where is the project amount stored?',\n",
    "             'Which data mart is responsible for CUSTOMER_360?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8919392-dbe6-47c0-9605-76ca06a355cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9022d7-5ddb-491b-9aaa-f78301d26f7a",
   "metadata": {},
   "source": [
    "TEST MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81c773f3-0994-4a0d-a027-b8a10fd427f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in models:\n",
    "    model = ChatOllama(model=model_name)\n",
    "    \n",
    "    for question in questions:\n",
    "        prompt = ChatPromptTemplate.from_template(\"\"\"You are an expert assistant for the Analytics Center.\n",
    "        Your task is to help the user work with the database: explain tables, generate SQL queries, optimize them, find errors, and suggest improvements.\n",
    "        Respond clearly, briefly, and in english. If you generate an SQL query — write it in a code block. If the user's request is unclear — ask a clarifying question.\n",
    "        \n",
    "        Analytics Center Description\n",
    "        Database: Vertica 24.1.0\n",
    "        Schemas:\n",
    "        \n",
    "        STAGE_DO – Temporary or primary storage of \"raw\" data loaded from sources. These are \"raw\" data loaded directly from sources, often without cleaning or normalization. Using it directly is risky: the data may be dirty, incomplete, or unstable.\n",
    "        \n",
    "        DWH – Data warehouse. This is a normalized, verified, and consistent data warehouse. This is usually the best choice: the data here has already been processed, cleaned, and standardized.\n",
    "        \n",
    "        DM – Data marts. These are aggregated, specialized datasets prepared for specific tasks or reports. Very convenient for targeted analytical queries, but not always suitable if detailed data is needed. Use tables from these schema first if it's possible\n",
    "        \n",
    "        SANDBOX – Sandbox: an isolated environment where analysts, data scientists, and developers can experiment with data without disrupting the core data warehouse architecture.\n",
    "        \n",
    "        Database metadata in JSON: {docs}\n",
    "        \n",
    "        Answer the question: \"\"\" + question\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Convert loaded documents into strings by concatenating their content\n",
    "        # and ignoring metadata\n",
    "        def format_docs(docs):\n",
    "            return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "        \n",
    "        \n",
    "        chain = {\"docs\": format_docs} | prompt | model | StrOutputParser()\n",
    "                \n",
    "        docs = vectorstore.similarity_search(question, 50)\n",
    "\n",
    "        response = chain.invoke(docs)\n",
    "\n",
    "        response_rus = ollama.chat(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {'role': 'user', 'content': f'Translate the text provided below into Russian. However, do not change the names of tables and columns. Table names start with STAGE_DO, DWH, DM, or SANDBOX. The names of both tables and columns are written in uppercase letters. \"{response}\"'}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        results.append(['nomic-embed-text', model_name, question, response, response_rus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adf098d4-f3b2-4138-af7a-bd8353a282e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "72d1ac9e-04e8-40e8-85e5-23c2eac7a042",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ed5a5b8d-26fd-490a-aef8-5d36650082dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns =['embedding_model','llm_model','question', 'response', 'response_rus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b4ee0fd0-3d0c-454a-adb2-8d5bf2578faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'C:\\Users\\test\\Desktop\\DBAssistant\\results.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b70ad3-4f71-4847-b033-776b6287f7f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
